{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77397980-6c11-4313-9259-85fad328026f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "from kafka import TopicPartition\n",
    "from kafka import KafkaProducer\n",
    "import json\n",
    "import time\n",
    "import  pandas as pd\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c22499db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5a8d6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../src/\")\n",
    "%pwd\n",
    "name_space = \"eabraham-373705\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebc36f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.1-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "23/12/29 11:59:05 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/12/29 11:59:05 WARN spark.SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "\n",
    "LOCAL_IP = socket.gethostbyname(socket.gethostname())\n",
    "name_space = \"eabraham-373705\"\n",
    "\n",
    "# Master node\n",
    "kubernetes_master_url = \"k8s://https://10.32.7.103:6443\"\n",
    "\n",
    "# Resource settings\n",
    "driver_cores = \"1\"\n",
    "executor_cores = \"1\"\n",
    "driver_memory = \"3g\"\n",
    "executor_memory = \"3g\"\n",
    "executor_memory_overhead = \"0.5g\"\n",
    "\n",
    "# These are the limits\n",
    "cpu_limit = \"3\"  # 12 cores\n",
    "memory_limit = \"32g\"  # Upto 32 GB\n",
    "executor_limit = \"8\"\n",
    "\n",
    "APP_NAME = 'scalables_executor'\n",
    "\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(APP_NAME)\\\n",
    "    .master(kubernetes_master_url)\\\n",
    "    .config(\"spark.driver.host\", LOCAL_IP)\\\n",
    "    .config(\"spark.driver.bindAddress\", \"0.0.0.0\")\\\n",
    "    .config(\"spark.executor.instances\", \"2\")\\\n",
    "    .config(\"spark.executor.cores\", executor_cores)\\\n",
    "    .config(\"spark.executor.memory\", executor_memory)\\\n",
    "    .config(\"spark.memory.fraction\", \"0.8\")\\\n",
    "    .config(\"spark.memory.storageFraction\", \"0.2\")\\\n",
    "    .config(\"spark.kubernetes.executor.limit.cores\", executor_limit)\\\n",
    "    .config(\"spark.kubernetes.namespace\", name_space)\\\n",
    "    .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\", \"spark\")\\\n",
    "    .config(\"spark.kubernetes.driver.label.appname\", APP_NAME)\\\n",
    "    .config(\"spark.kubernetes.executor.label.appname\", APP_NAME)\\\n",
    "    .config(\"spark.kubernetes.executor.deleteOnTermination\", \"false\") \\\n",
    "    .config(\"spark.kubernetes.container.image.pullPolicy\", \"Always\") \\\n",
    "    .config(\"spark.kubernetes.container.image\", \"node03.st:5000/pyspark-hdfs-jupyter:eabraham-373705-v4-executor\")\\\n",
    "    .config(\"spark.local.dir\", \"/tmp/spark\")\\\n",
    "    .config(\"spark.kubernetes.driver.volumes.emptyDir.spark-local-dir-tmp-spark.mount.path\", \"/tmp/spark\")\\\n",
    "    .config(\"spark.kubernetes.driver.volumes.emptyDir.spark-local-dir-tmp-spark.mount.readOnly\", \"false\")\\\n",
    "    .config(\"spark.kubernetes.executor.volumes.emptyDir.spark-local-dir-tmp-spark.mount.path\", \"/tmp/spark\")\\\n",
    "    .config(\"spark.kubernetes.executor.volumes.emptyDir.spark-local-dir-tmp-spark.mount.readOnly\", \"false\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a635a11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_serializer(data):\n",
    "    return json.dumps(data).encode(\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a902971",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../src/test_data/sample_data.csv')\n",
    "data = data.drop(columns = ['Unnamed: 0'])\n",
    "data = data.fillna('None')\n",
    "for column in data:\n",
    "    data[column] = data[column].astype('str')\n",
    "\n",
    "prices = data[['vin', 'price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de32de84",
   "metadata": {},
   "outputs": [],
   "source": [
    "producer = KafkaProducer(bootstrap_servers=\"kafka-service:9092\",\n",
    "                         value_serializer=json_serializer)\n",
    "i = 0\n",
    "for row in data.to_dict('records'): \n",
    "    #print(row)\n",
    "    producer.send(\"Objects\", row)\n",
    "    i += 1\n",
    "    if i >10:\n",
    "        break\n",
    "        \n",
    "j = 0\n",
    "for row in prices.to_dict('records'): \n",
    "    #print(row)\n",
    "    producer.send(\"Price\", row)\n",
    "    j += 1\n",
    "    if i >10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8994e798-e57f-49a5-8a78-a188f239c784",
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer = KafkaConsumer(\n",
    "        #\"Objects\",\n",
    "        bootstrap_servers=\"kafka-service:9092\",\n",
    "        auto_offset_reset='earliest',\n",
    "        #group_id=\"consumer-group-a\"\n",
    "        )\n",
    "tp = TopicPartition('Objects', 0)\n",
    "#register to the topic\n",
    "consumer.assign([tp])\n",
    "\n",
    "# obtain the last offset value\n",
    "consumer.seek_to_end(tp)\n",
    "lastOffset = consumer.position(tp)\n",
    "consumer.seek_to_beginning(tp)\n",
    "msg_list = []\n",
    "for msg in consumer:\n",
    "        msg_list.append(json.loads(msg.value))\n",
    "        if msg.offset == lastOffset - 1:\n",
    "            df = spark.createDataFrame(msg_list)\n",
    "            break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e553f6d2-01d8-47e8-af0b-7c5190f75d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "   \n",
    "for i in range(5):\n",
    "    consumer = KafkaConsumer(\n",
    "        bootstrap_servers=\"kafka-service:9092\",\n",
    "        auto_offset_reset='earliest')\n",
    "\n",
    "    tp = TopicPartition('Objects', i)\n",
    "    consumer.assign([tp])\n",
    "\n",
    "    # obtain the last offset value\n",
    "    consumer.seek_to_end(tp)\n",
    "    lastOffset = consumer.position(tp)\n",
    "    consumer.seek_to_beginning(tp)\n",
    "    msg_list = []\n",
    "    for msg in consumer:\n",
    "        msg_list.append(json.loads(msg.value))\n",
    "        #print(\"Message = {}\".format(json.loads(msg.value)))\n",
    "        if msg.offset == lastOffset - 1:\n",
    "            otp = spark.createDataFrame(msg_list)\n",
    "            df = df.union(otp)\n",
    "            df = df.distinct()\n",
    "            break\n",
    "df.write.mode('overwrite').format('parquet').save('hdfs:///home/eabraham-373705/data/raw/NEW_raw_data.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25d4ea13",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = spark.read.parquet('/home/eabraham-373705/data/predictions/NEW_second_level_predictions.parquet', \n",
    "                        header=True, inferSchema=True)\n",
    "pred = pred.toPandas()\n",
    "for row in pred.to_dict('records'): \n",
    "    #print(row)\n",
    "    producer.send(\"Predictions\", row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e60bf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
